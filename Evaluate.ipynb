{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Nilanshrajput/AdvitiyaHackathon/blob/master/Bert_finetuned_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "colab_type": "code",
    "id": "447jWka-yLBL",
    "outputId": "a571545f-49b0-4191-afe6-8dc37ac8d3c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
      "\u001b[K     |████████████████████████████████| 481kB 3.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 18.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 19.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
      "Collecting tokenizers==0.0.11\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1MB 28.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.9 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.9)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (2.6.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (0.15.2)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=530f8ad773a48051f0c813ebbfb8b47ac6afccbb224ec86a501bd11848cec6d7\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "colab_type": "code",
    "id": "5hYcPCU-4Jkm",
    "outputId": "119d5b39-1958-45c2-c482-d54761b3feee"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "import re \n",
    "import matplotlib.pyplot as plt\n",
    "# ~ Import packages ~ #\n",
    "from AdvitiyaHackathon.google_patent_scrapper import scraper_class\n",
    "import json\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import spacy #load spacy\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "-ZJfAJDPN9dC",
    "outputId": "054fbab6-ce41-49fa-dd71-c9d8380a436d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4==4.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tnrange\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_wrangler:\n",
    "  def __init__(self, data_csv_pt=None,patent = None, eval = False, save = True, max_limit = 10000, classification = True, abstract =False, claims  = False, all = False,\n",
    "               path = \"/content/drive/My Drive/dl_projects/AdvitiyaHackathon/ScrappeData\",\n",
    "               test_path = \"/content/drive/My Drive/dl_projects/AdvitiyaHackathon/Test_scrap\",\n",
    "               val = False, test = False):\n",
    "    self.patent_info = \"\"\n",
    "    self.path = path\n",
    "    self.patent =patent\n",
    "    self.eval = eval\n",
    "    self.save = save\n",
    "    self.test_path = test_path\n",
    "    self.max_limit = max_limit\n",
    "    self.class_names = [\"non_alc\", \"alc\", \"non_auto\", \"auto\"]\n",
    "    self.info_classes = [\"title\", 'classification' ]\n",
    "    self.data_csv_pt = data_csv_pt\n",
    "    self.csv  = pd.read_csv(self.data_csv_pt,index_col= 0)\n",
    "    \n",
    "    self.scraper = scraper_class()\n",
    "    \n",
    "\n",
    "    self.nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
    "    self.nlp.max_length = 50000000000\n",
    "    self.stops = stopwords.words(\"english\")\n",
    "    self.punctuations = string.punctuation\n",
    "\n",
    "    self.cleaned_data  = pd.DataFrame(columns= [\"text\",\"cat\"])\n",
    "    self.cleaned_data_test  = pd.DataFrame(columns= [\"text\",\"cat\"])\n",
    "\n",
    "    if abstract :\n",
    "      self.info_classes = [\"title\", \"abstract\", \"claims\", 'classification' ]\n",
    "    elif claims :\n",
    "      self.info_classes = [\"title\", \"claims\", 'classification', \"abstract\" ]\n",
    "    elif classification :\n",
    "      self.info_classes = [\"title\", 'classification', \"abstract\", \"claims\" ]\n",
    "\n",
    "    else :\n",
    "      self.info_classes = [\"title\", 'classification', \"abstract\", \"claims\" ]\n",
    "    \n",
    "    if eval :\n",
    "      self.scrappedJson = self.scraper.scrap_single_patent(patent)\n",
    "      try:  \n",
    "        self.patent_info = \"\"\n",
    "        for info in self.info_classes:\n",
    "          self.patent_info += self.scrappedJson[info]\n",
    "          if len(self.patent_info.split()) > self.max_limit:\n",
    "            break\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "      self.patent_info = self.clean(self.patent_info)\n",
    "      self.patent_info = re.sub(' +', ' ',str(self.patent_info))\n",
    "  else:\n",
    "\n",
    "      if test:\n",
    "        self.test_csv = pd.read_csv(self.data_csv_pt)\n",
    "        self.test_nums = list(self.test_csv.iloc[:,0])\n",
    "        self.test_json_pt = [os.path.join(self.test_path,p_num+\".json\") for p_num in self.test_nums]\n",
    "        try: \n",
    "        \n",
    "          for patent in tqdm(self.test_json_pt,desc  = \"patents\"):\n",
    "              with open(patent) as file:\n",
    "                  data = json.load(file)\n",
    "              try:  \n",
    "                self.patent_info = \"\"\n",
    "                for info in self.info_classes:\n",
    "                  self.patent_info += data[info]\n",
    "                  if len(self.patent_info.split()) > self.max_limit:\n",
    "                    break\n",
    "              except Exception as e:\n",
    "                print(e)\n",
    "              self.patent_info = self.clean(self.patent_info)\n",
    "              self.patent_info = re.sub(' +', ' ',str(self.patent_info))\n",
    "              self.cleaned_data_test = self.cleaned_data_test.append({\"text\":str(self.patent_info),\"cat\":0},ignore_index = True)\n",
    "        \n",
    "\n",
    "        except Exception as e:\n",
    "          print(e)\n",
    "\n",
    "        self.cleaned_data_test.to_csv(\"final_test_clean.csv\")\n",
    "      \n",
    "\n",
    "        \n",
    "      else:\n",
    "\n",
    "        # Make a separate list for each category\n",
    "        self.non_alc = list(self.csv.iloc[:,0])\n",
    "        self.alc = list(self.csv.iloc[:,2])\n",
    "        self.non_auto = list(self.csv.iloc[:,4])\n",
    "        self.auto = list(self.csv.iloc[:,6])\n",
    "\n",
    "        self.catwise_patent_lists  = [self.non_alc, self.alc, self.non_auto, self.auto]\n",
    "\n",
    "        # Make a separate list for each category\n",
    "        self.non_alc_json_pt = [os.path.join(self.path,self.class_names[0],p_num+\".json\") for p_num in self.catwise_patent_lists[0]]\n",
    "        self.alc_json_pt = [os.path.join(self.path,self.class_names[1],p_num+\".json\") for p_num in self.catwise_patent_lists[1]]\n",
    "        self.non_auto_json_pt = [os.path.join(self.path,self.class_names[2],p_num+\".json\") for p_num in self.catwise_patent_lists[2]]\n",
    "        self.auto_json_pt = [os.path.join(self.path,self.class_names[3],p_num+\".json\") for p_num in self.catwise_patent_lists[3]]\n",
    "\n",
    "      \n",
    "        i = 0\n",
    "        for pt_list in tqdm([self.non_alc_json_pt,self.alc_json_pt,self.non_auto_json_pt, self.auto_json_pt], desc = \"CatList\"):\n",
    "        \n",
    "          try: \n",
    "          \n",
    "            for patent in tqdm(pt_list,desc  = \"patents\"):\n",
    "                with open(patent) as file:\n",
    "                    data = json.load(file)\n",
    "                try:  \n",
    "                  self.patent_info = \"\"\n",
    "                  for info in self.info_classes:\n",
    "                    self.patent_info += data[info]\n",
    "                    if len(self.patent_info.split()) > self.max_limit:\n",
    "                      break\n",
    "                except Exception as e:\n",
    "                  print(e)\n",
    "                self.patent_info = self.clean(self.patent_info)\n",
    "                self.patent_info = re.sub(' +', ' ',str(self.patent_info))\n",
    "                self.cleaned_data = self.cleaned_data.append({\"text\":str(self.patent_info),\"cat\":i},ignore_index = True)\n",
    "            i += 1\n",
    "\n",
    "          except Exception as e:\n",
    "            print(e)\n",
    "      if self.save: \n",
    "          if val:\n",
    "            self.cleaned_data.to_csv(\"final_val_clean.csv\")\n",
    "          else:\n",
    "            self.cleaned_data.to_csv(\"final_train_clean.csv\")\n",
    "\n",
    "  def get_data(self):\n",
    "    if self.eval:\n",
    "      return self.patent_info\n",
    "    else:\n",
    "      return self.cleaned_data\n",
    "\n",
    "  def clean(self, comment, lowercase = True, remove_stopwords = True):\n",
    "    if lowercase:\n",
    "        comment = comment.lower()\n",
    "    comment = self.nlp(comment,disable = ['ner', 'parser', 'tagger'])\n",
    "    lemmatized = list()\n",
    "    for word in comment:\n",
    "        lemma = word.lemma_.strip()\n",
    "       \n",
    "        #len(re.findall('[^A-Za-z0-9]+',lemma))==0 for removing alphanumerals in b/w strings\n",
    "\n",
    "        if lemma:\n",
    "            if not remove_stopwords or (remove_stopwords and lemma not in self.stops and not (\"\\\\\" in r\"%r\" % lemma)  ):\n",
    "                lemma = re.sub(\"[^a-zA-Z]\",  # Search for all non-letters\n",
    "                          \" \",          # Replace all non-letters with spaces\n",
    "                          str(lemma))\n",
    "                lemmatized.append(lemma)\n",
    "    return \" \".join(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Evaluate:\n",
    "    def __init__(self,model_paths):\n",
    "        \n",
    "        self.model_paths = model_paths\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "                        \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "                        num_labels = 4, # The number of output labels--2 for binary classification.\n",
    "                                        # You can increase this for multi-class tasks.   \n",
    "                        output_attentions = True, # Whether the model returns attentions weights.\n",
    "                        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "                    )\n",
    "        self.model.cuda()\n",
    "        \n",
    "        # If there's a GPU available...\n",
    "        if torch.cuda.is_available():    \n",
    "\n",
    "            # Tell PyTorch to use the GPU.    \n",
    "            self.device = torch.device(\"cuda\")\n",
    "\n",
    "            print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "            print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "        # If not...\n",
    "        else:\n",
    "            print('No GPU available, using the CPU instead.')\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        \n",
    "        self.models = [self.model for i in range(len(self.model_paths))]\n",
    "        i=0\n",
    "        for path in tqdm(self.model_paths, desc = \"Loading Models\"):\n",
    "            # Loading the checkpoints for resuming training\n",
    "            checkpoint = torch.load(path)\n",
    "            models[i].load_state_dict(checkpoint['model_state_dict'])\n",
    "            i+=1\n",
    "            \n",
    "            models[i].eval()\n",
    "            \n",
    "            \n",
    "    def predict(self, patent):\n",
    "        self.patent_list = patent\n",
    "        self.extractor  = data_wrangler(eval = True,patent = patent,save=False)\n",
    "        sentence = self.extractor.get_data()\n",
    "        b_input_ids, b_input_mask = self.pre_process(sentence, self.maxlen = 512,device=self.device)\n",
    "        results = []\n",
    "        for model in tqdm(self.models):\n",
    "            output = model(b_input_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=b_input_mask)\n",
    "            logits = output[0]\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            cat = np.argmax(logits, axis=1).flatten()\n",
    "            results.append(int(cat))\n",
    "        cat  = self.most_frequent(results)\n",
    "        return cat\n",
    "\n",
    "    def most_frequent(self, List): \n",
    "        counter = 0\n",
    "        num = List[0] \n",
    "\n",
    "        for i in List: \n",
    "            curr_frequency = List.count(i) \n",
    "            if(curr_frequency> counter): \n",
    "                counter = curr_frequency \n",
    "                num = i \n",
    "\n",
    "        return num \n",
    "\n",
    "\n",
    "\n",
    "    def pre_process(self, sentence, maxlen,device):\n",
    "      tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "      #sentence = sentence[50:]\n",
    "      tokens_ids = tokenizer.encode(str(sentence),add_special_tokens = True,max_length = maxlen, pad_to_max_length=True)\n",
    "      # Converting the list to a pytorch tensor\n",
    "      tokens_ids_tensor = torch.tensor(tokens_ids)\n",
    "\n",
    "      # Obtaining the attention mask\n",
    "      attn_mask = (tokens_ids_tensor != 0).long()\n",
    "\n",
    "      return tokens_ids_tensor.unsqueeze(dim = 0).to(device), attn_mask.unsqueeze(dim = 0).to(device)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7TlkavJOGYt"
   },
   "outputs": [],
   "source": [
    "drive_pt=\"/content/drive/My Drive/dl_projects/AdvitiyaHackathon/\"\n",
    "\n",
    "git_path = \"AdvitiyaHackathon/\"\n",
    "path = drive_pt+\"ScrappeData\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Bert_finetuned_V1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
